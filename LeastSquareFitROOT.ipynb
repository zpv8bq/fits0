{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Fitting\n",
    "This example describes a linear least-squares analysis.\n",
    "  * The code is loosely based on the example: https://root.cern/doc/master/solveLinear_8C.html\n",
    "  * Many detailed references are avilaible for the least squares technique, for example see: http://vmls-book.stanford.edu/vmls.pdf\n",
    "\n",
    "Linear regression can be used to solve for ($k$) unknown parameters ($\\theta$) in a model of the form:\n",
    "\n",
    "$\\hat f(x; \\vec\\theta) = \\theta_1*f_1(x) + \\theta_2*f_2(x) + \\theta_3*f_3(x) + ... + \\theta_k*f_k(x)$,<br>\n",
    "where the $f_i(x)$ can be arbitrary functions of $x$, but each term $\\theta_i*f_i(x)$ can only depend linearly on the parameters, $\\frac{\\partial^2 f(x)}{\\partial \\theta_{i}^2} = 0$.<br>\n",
    "For a given $x$, our predicted outcome is $\\hat y = \\hat f(x)$.\n",
    "\n",
    "Let's assume we have vectors of $n$ measurements $x_i$, then $y_i \\equiv \\hat f(x_i) = \\hat y_i$, where the equivalence is true to a good approximatation if the model is a good representation of our data.  To find the model $\\hat f$ that it is most consistent with our data, we will minimze (a function of) the residuals for each data point $r_i=y_i−\\hat y_i$.\n",
    "\n",
    "Define the vectors: \n",
    "  * $y^d= (y_1,...,y_n)$ : values of dependent variables measured in data, our observations\n",
    "  * $\\hat y^d= (\\hat y_1,...,\\hat y_n)$ : expected values of the observations, given our model\n",
    "  * $r^d= (r_1,...,r_n)$ : residuals between the data and model\n",
    "  \n",
    "For a least squares fitting model, we want to find the parameters $\\vec\\theta$ that minimize the sum of squares of the prediction error.  In performing this calculation we will weight the residuals by the inverse of the uncertainy in each measurement.  As discussed in class, the uncertainties follow a normal distribution and this is equivalent to minimizing the $\\chi^2$ value.\n",
    "\n",
    "Each measurement gives us information about the unknown parameters $\\vec \\theta$.  We can write the expectation for our model for each $x_i$ as:<br>\n",
    "$\\hat f(x_i; \\vec \\theta) = \\hat y(x_i)= A_{i1}\\theta_1 + A_{i2}\\theta_2 + ... + A_{ik}\\theta_k,~ i=1,...,n$,<br>\n",
    "where we define the $n×k$ matrix $A$ as $A_{ij}=f_j(x_i),  i= 1,...,n, ~ j= 1,...,k$.\n",
    "\n",
    "Writing this in matrix notation gives :<br>\n",
    "$\\hat y^d = {\\bf A}\\,\\vec θ$\n",
    "\n",
    "The sum of squares of the residuals is then \n",
    "$|r^d|^2 = |y^d−\\hat y^d|^2 = |y^d−{\\bf A}\\,\\vec θ|^2$, which is the quantity we want to minimze by requiring:\n",
    "$\\frac{\\partial |r^d|^2} {\\partial \\theta_i} = \\frac{\\partial  |y^d−{\\bf A}\\,\\vec θ|^2 } {\\partial \\theta_i} = 0 $.\n",
    "\n",
    "# Example: solution to linear fit for 1st order polynomial: y = a + bx\n",
    "\n",
    "Where we include the following four data points in the fit:<br>\n",
    "$x$ = {0.0,1.0,2.0,3.0}<br>\n",
    "$y^d$ = {1.4,1.5,3.7,4.1}<br>\n",
    "$\\sigma^d$ = {0.5,0.2,1.0,0.5}\n",
    "\n",
    "Using the notation above, we have \n",
    "$\\hat y^d = {\\bf A}\\,\\vec θ$, and writing out the terms explicitly gives:\n",
    "\n",
    "$\\vec θ = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$, $f_1(x_i) = 1, f_2(x_i)=x_i$, eg. $y = a + bx$<br>\n",
    "$y^d = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{pmatrix}$, \n",
    "${\\bf A} =\n",
    "\\begin{pmatrix} \n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22} \\\\ \n",
    "A_{31} & A_{32} \\\\\n",
    "A_{41} & A_{42}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "f_1(x_1) & f_2(x_1) \\\\ \n",
    "f_1(x_2) & f_2(x_2) \\\\ \n",
    "f_1(x_3) & f_2(x_3) \\\\ \n",
    "f_1(x_4) & f_2(x_4) \n",
    "\\end{pmatrix}$\n",
    "\"N_parameters\" wide, \"N_data_points\" deep.\n",
    "\n",
    "To be explicit, $\\hat y^d = {\\bf A}\\,\\vec θ$, so:\n",
    "\n",
    "$ \\begin{pmatrix} \n",
    "f_1(x_1) & f_2(x_1) \\\\ \n",
    "f_1(x_2) & f_2(x_2) \\\\ \n",
    "f_1(x_3) & f_2(x_3) \\\\ \n",
    "f_1(x_4) & f_2(x_4) \n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix} a \\\\ b \\end{pmatrix} = $\n",
    "$\\begin{pmatrix} \n",
    "a + bx_1 \\\\ \n",
    "a + bx_2 \\\\ \n",
    "a + bx_3 \\\\ \n",
    "a + bx_4 \\\\ \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix} \n",
    "\\hat y_1 \\\\ \n",
    "\\hat y_2 \\\\ \n",
    "\\hat y_3 \\\\ \n",
    "\\hat y_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "=\\hat y^d $\n",
    "\n",
    "and using:\n",
    "\n",
    "$|r^d|^2 = |y^d - \\hat y^d|^2 = |y^d−{\\bf A}\\,\\vec θ|^2$, where we require: $\\frac{\\partial |r^d|^2} {\\partial \\theta_i} =0 $\n",
    "\n",
    "we have the following system of equations to solve:\n",
    "\n",
    "$ 0 = -2 \\sum_i [ y^d_i - f_1(x_i)\\theta_1 - f_2(x_i)\\theta_2 ] f_1(x_i) $ <br>\n",
    "$ 0 = -2 \\sum_i [ y^d_i - f_1(x_i)\\theta_1 - f_2(x_i)\\theta_2 ] f_2(x_i) $ <br>\n",
    "or $ 0 = (y^d - {\\bf A}\\vec\\theta){\\bf A}^T$ <br>\n",
    "For $\\chi^2$ minimization, we just multiply ${\\bf A}$ and $y^d$ by $\\frac{1}{\\sigma^d}$.  In other words $|r^d|^2=\\chi^2$.\n",
    "\n",
    "Provided the columns of ${\\bf A}$ are linearly independent, we can solve this least squares problem to find $\\vec\\theta$, the model parameter values that minimize above prediction error for our data set, using:<br>\n",
    "$\\vec\\theta = ({\\bf A}^T{\\bf A})^{-1} {\\bf A}^T y^d={\\bf A}^\\dagger y^d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp -d\n",
    "#include \"Riostream.h\"\n",
    "#include \"TMatrixD.h\"\n",
    "#include \"TVectorD.h\"\n",
    "#include \"TGraphErrors.h\"\n",
    "#include \"TF1.h\"\n",
    "#include \"TH1F.h\"\n",
    "#include \"TH2F.h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SolveLSQ returns:\n",
    "$\\vec\\theta = ({\\bf A}^T{\\bf A})^{-1} {\\bf A}^T y^d={\\bf A}^\\dagger y^d$<br>\n",
    "For the $\\chi^2$ minimization, ${\\bf A}$ and $y^d$ are assumed to be weighted by the measurement uncertinties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp -d\n",
    "TMatrixD SolveLSQ(const TMatrixD &A, const TMatrixD &y){\n",
    "  TMatrixD AT=(A);  \n",
    "  AT.T();            // A transpose\n",
    "  TMatrixD ATAi(AT,TMatrixD::kMult,A);\n",
    "  ATAi.Invert();\n",
    "  TMatrixD Adag(ATAi,TMatrixD::kMult,AT);  // (A^T A)^(-1) A^T\n",
    "  TMatrixD theta(Adag,TMatrixD::kMult,y);  // (A^T A)^(-1) A^T * y\n",
    "  return theta;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cpp\n",
    "cout << \"Perform the fit  y = a + b * x \" << endl;\n",
    "\n",
    "const Int_t nPar  = 2;\n",
    "\n",
    "Double_t ax[] = {0.0,1.0,2.0,3.0};  // x_i\n",
    "Double_t ay[] = {1.4,1.5,3.7,4.1};  // y_i\n",
    "Double_t ae[] = {0.5,0.2,1.0,0.5};  // sigma_i\n",
    "const Int_t nPnts = sizeof(ax)/sizeof(Double_t);\n",
    "auto tge=new TGraphErrors(sizeof(ax)/sizeof(Double_t),ax,ay,0,ae);\n",
    "auto tc=new TCanvas();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "tge->Draw(\"alp*\");\n",
    "tge->SetLineColor(kGreen);\n",
    "tc->Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "// Define vectors and matrices.<br>\n",
    "// Make the vectors 'use' the data : they are not copied, \n",
    "// the vector data pointer is just set appropriately\n",
    "TVectorD x; x.Use(nPnts,ax);\n",
    "TVectorD y; y.Use(nPnts,ay);\n",
    "TVectorD e; e.Use(nPnts,ae);\n",
    "\n",
    "TMatrixD A(nPnts,nPar);       // A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "// Fill the A matrix\n",
    "TMatrixDColumn(A,0) = 1.0;\n",
    "TMatrixDColumn(A,1) = x;\n",
    "cout << \"A = \";\n",
    "A.Print();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "// apply weights\n",
    "TMatrixD yw(A.GetNrows(),1);   // error-weighted data values\n",
    "\n",
    "for (Int_t irow = 0; irow < A.GetNrows(); irow++) {\n",
    "    TMatrixDRow(A,irow) *= 1/e(irow);\n",
    "    TMatrixDRow(yw,irow) += y(irow)/e(irow);\n",
    "}\n",
    "cout << \"A weighted = \";\n",
    "A.Print();\n",
    "cout << \"y weighted = \";\n",
    "yw.Print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "TMatrixD theta=SolveLSQ(A, yw);\n",
    "cout << \"Param vector = \";\n",
    "theta.Print();\n",
    "TF1 *fn1 = new TF1(\"fn1\",\"[0]+[1]*x\",0,3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "// View the resulting fit\n",
    "fn1->SetParameters(theta[0][0],theta[1][0]);\n",
    "tge->Draw(\"alp*\");\n",
    "fn1->Draw(\"same\");\n",
    "tc->Draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and add more data points to the data and see how the code adapts to perform your fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "\n",
    "The functions `getX` and `getY` below are used to generate a random data distribution following a function of logarithmic values.\n",
    "\n",
    "$f(x) = a + b*log(x) + c*log(x)*log(x)$\n",
    "\n",
    "The y-values are randomly generated.  You can run execute the code multiple times to see the distribution changing.\n",
    "\n",
    "Your task is to use the least squares fittng technique using the function above to extract the parameters a, b, c from the fit, based on the generated x, y, ey values.\n",
    "\n",
    "Once you have the fit working, perform the following studies:\n",
    "\n",
    " * Repeat the fit a large number of times on different data sets\n",
    " * Create a 2x2 panel canvas and plot\n",
    "   * (1--3) the distribution of values of a, b, c in three histograms\n",
    "   * (4) calculate the chi^2 for each fit and plot the distribution of the $\\chi^2$ over the experiments\n",
    " * Observe how the extracted parameters fluctuate.  How is this result affected if you increase/decrease the number of points in your experiment or increase/decrease the size of the uncertainties?\n",
    " * Compare the mean and standard deviation of the chi2 distribution to expectations\n",
    " * In a second 2x2 canvas plot\n",
    "   * (1) The values of parameter $b$ vs $a$ over the experiments (2D histogram, use the 'colz' plotting option)\n",
    "   * (2) The values of parameter $c$ vs $a$ over the experiments \n",
    "   * (3) The values of parameter $c$ vs $b$ over the experiments \n",
    "   * (4) The distribution of the *reduced* $\\chi^2$ over the experiments \n",
    "\n",
    "Place your plots and comments into a single PDF file called `LSQFit.pdf` and upload this along with your work to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an illustration of the pseudoexperiments that are generated in the `LSQFit` program that you will modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp \n",
    "// this cell may be run multiple times to change parameters\n",
    "#include \"TMath.h\"\n",
    "// parms\n",
    "const double xmin=1;\n",
    "const double xmax=20;\n",
    "const int npoints=12;\n",
    "const int maxpoints=100;\n",
    "const double sigma=0.2;\n",
    "double lx[maxpoints];  // simple choise to avoid dynamic memory allocation\n",
    "double ly[maxpoints];\n",
    "double ley[maxpoints];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp -d\n",
    "// this cell may only be run once without restarting the kernel\n",
    "\n",
    "Double_t f(Double_t *x, Double_t *par){\n",
    "  return par[0]+par[1]*TMath::Log(x[0])+par[2]*TMath::Log(x[0])*TMath::Log(x[0]);\n",
    "}\n",
    "\n",
    "double f(double x){\n",
    "  double pars[]={0.5,1.3,0.5};  // a,b,c\n",
    "  return f(&x,pars);\n",
    "}\n",
    "\n",
    "void getX(double *x){\n",
    "  double step=(xmax-xmin)/npoints;\n",
    "  for (int i=0; i<npoints; i++){\n",
    "    x[i]=xmin+i*step;\n",
    "  }\n",
    "}\n",
    "\n",
    "void getY(const double *x, double *y, double *ey){\n",
    "  static TRandom2 tr(0);\n",
    "  for (int i=0; i<npoints; i++){\n",
    "    y[i]=f(x[i])+tr.Gaus(0,sigma);\n",
    "    ey[i]=sigma;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cpp\n",
    "// get a random sampling of the (x,y) data points, rerun to generate different data sets for the plot below\n",
    "getX(lx);\n",
    "getY(lx,ly,ley);\n",
    "auto tgl = new TGraphErrors(npoints,lx,ly,0,ley);\n",
    "// An example of one pseudo experiment\n",
    "tgl->Draw(\"alp\");\n",
    "tc->Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of getting using the C++ variables the Python side\n",
    "R.tgl.Draw(\"alp\");\n",
    "R.tc.Draw();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
