{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import ROOT as R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sampling a PDF (probability density function) to generate pseudo data/pseudo experiments\n",
    "====\n",
    "\n",
    "**Create a PDF**  <br>\n",
    "We'll use our PDF to create *pseudo experiment* data distributions to test various tools that can be applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "tc=R.TCanvas()\n",
    "pi=R.TMath.Pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xmin=-pi/16; xmax=1.2*pi\n",
    "# Create a function with 1 free parameter. We'll use this as our PDF\n",
    "fsin = R.TF1(\"fsin\", \"[0]*(sin(x)*sin(x)+sin(1.5*x)*sin(1.5*x))*exp(-x/2)\", xmin, xmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fsin.SetParameter(0,1)\n",
    "fsinNorm=1.0/fsin.Integral(xmin,xmax)\n",
    "fsin.SetParameter(0,fsinNorm)  # normalize to unit area\n",
    "fsin.SetMinimum(0)   # fix minimum value of y-axis to 0\n",
    "fsin.SetTitle(\"Some PDF: fsin;x;PDF(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fsin.Draw()\n",
    "tc.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Generate some pseudodata using our PDF** <br>\n",
    "You can think about what we will do in the following way.  The function **fsin** represents some model of a *true* underlying process.  We perform an experiment that measures the process a discrete number of times (**ntrials** below).  \n",
    "\n",
    "An experiment (even a perfect experiment with no measurement errors) can never fully measure the *true* or *parent* probability distribution.  Through experiments, we can sample the *parent* probability distribution, and our observation is called the *sample distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* With a small number of samples, statistical uncertainties will dominate the precision of our observations.  \n",
    "* With a very large number of samples, the sample distribution will approach the parent distribution.  An important caveat is that imperfections in our experimental appararus will eventually become the dominant source of our uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# generate a pseudoexperiment (PE) with ntrials samples\n",
    "# call this function to generate a new PEs\n",
    "def GenPE(ntrials=1000,nbins=10):\n",
    "    hpdata=R.TH1F(\"hpdata\",\"Pseudo data\",nbins,xmin,xmax)\n",
    "    hpdata.SetMinimum(0)\n",
    "    for i in range(ntrials): hpdata.Fill(fsin.GetRandom(xmin,xmax))\n",
    "    return hpdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example of a pseudo experiment composed of randomly generated data\n",
    "# rerun this cell to generate a new PE\n",
    "hpdata=GenPE()\n",
    "hpdata.Draw(\"e\")\n",
    "tc.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Next let's overlay the expectation for the true distribution on this pseudoexperiment.**\n",
    "\n",
    "Normalizations can be a little tricky when scaling your PDFs at match a certain normaliation in histogrammed data.  This is because the number of bins choosen affects the height of the data distributions, for example if you double the number of bins, the high is the histogram bins will be reduced by ~a factor of 2.  Below the normalization of the pdf is corrected as follows:\n",
    "* Extract the histogram that is used to draw the function and normalize it to unit area.\n",
    "* Scale this histgram by *ntrials* and also by the ratio of bins in the used to draw the function and the data.\n",
    "* Now any changes in the numebr bins used to plot the data, the number of trials, or the number of points used to plot the function (see [TH1F::SetNPx()](https://root.cern.ch/doc/master/classTF1.html#aea3c928ccaf8b32b1554d2b84158238e)) will automatically be corrected to get the correct normalization for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def NormPDF(hpdata,pdf):\n",
    "    fplot=pdf.GetHistogram()\n",
    "    fplot.Scale(1./fplot.Integral())\n",
    "    fplot.Scale(hpdata.Integral()*fplot.GetNbinsX()/hpdata.GetNbinsX())\n",
    "    return fplot\n",
    "\n",
    "def PlotPDvPDF(hpdata,pdf,tc):\n",
    "    fplot=NormPDF(hpdata,pdf)\n",
    "    hpdata.Draw(\"e\")\n",
    "    hpdata.Draw(\"e\")\n",
    "    fplot.Draw(\"same,hist,c\")\n",
    "    tc.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PlotPDvPDF(hpdata,fsin,tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now let's generate a new pseudodata set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hpdata=GenPE()\n",
    "PlotPDvPDF(hpdata,fsin,tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interpolations\n",
    "====\n",
    "\n",
    "We'll call some C++ code to implement Lagrange interpolation.  The order of the interpolation can be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "R.gROOT.ProcessLine(\".L Lagrange.cpp+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "orders=[1,2,5,8]  # different orders of interpolation to test\n",
    "#order=1 # linear interpolation\n",
    "#order=3 # cubic interpolation\n",
    "tgi=[]   # python list to be filled with different TGraphs\n",
    "\n",
    "# our interpolation function requires us to x,y arrays instead of a histogram\n",
    "x=np.zeros(hpdata.GetNbinsX(),dtype=np.double)\n",
    "y=np.zeros(hpdata.GetNbinsX(),dtype=np.double)\n",
    "ex=np.zeros(hpdata.GetNbinsX(),dtype=np.double)\n",
    "ey=np.zeros(hpdata.GetNbinsX(),dtype=np.double)\n",
    "for i in range(hpdata.GetNbinsX()):\n",
    "    x[i]=hpdata.GetBinCenter(i+1)\n",
    "    y[i]=hpdata.GetBinContent(i+1)\n",
    "    ex[i]=hpdata.GetBinWidth(i+1)/2\n",
    "    ey[i]=hpdata.GetBinError(i+1)\n",
    "    \n",
    "nInterp=100 # number of points for calculation of interpolated values\n",
    "step=(x[-1]-x[0])/nInterp  # step sixe for intepolated values\n",
    "\n",
    "# these arrays will store the x,y values for the interpolated data\n",
    "xi=np.array(range(nInterp),dtype=np.double)\n",
    "xi=xi*step+x[0]\n",
    "yi=np.zeros(nInterp,dtype=np.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# scan different orders of the Lagrange interpolation and save the reults in TGraphs\n",
    "for i in range(len(orders)): \n",
    "    for nx in range(nInterp):\n",
    "        yi[nx]=R.LgInterp(xi[nx],x,y,len(x),orders[i])\n",
    "    tgi.append(R.TGraph(len(xi),xi,yi))\n",
    "    tgi[i].SetLineColor(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot the results of our interpolations to the data and compare with the true distribution\n",
    "hpdata.SetStats(0)\n",
    "hpdata.SetMinimum(-hpdata.GetMaximum()/10)  # to show undershoot of large order interpolation\n",
    "hpdata.Draw(\"e\")\n",
    "tl=R.TLegend(0.7,0.5,0.9,0.9)\n",
    "for i in range(len(orders)): \n",
    "    tgi[i].Draw(\"l\")\n",
    "    tl.AddEntry(tgi[i],\"order=\"+str(orders[i]),\"l\")\n",
    "fplot=NormPDF(hpdata,fsin)\n",
    "fplot.SetLineStyle(2)\n",
    "fplot.SetLineColor(R.kYellow-3)\n",
    "tl.AddEntry(fplot,\"Truth\",\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fplot.Draw(\"same,hist,c\")\n",
    "tl.Draw()\n",
    "tc.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Examine the performace of a spline interpolation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# make a TGraph of our data\n",
    "grdata=R.TGraphErrors(len(x),x,y,ex,ey)\n",
    "for nx in range(nInterp):\n",
    "    yi[nx]=grdata.Eval(xi[nx],0,\"S\")  # built in 3-spline interpolation\n",
    "tgs=R.TGraph(len(xi),xi,yi)\n",
    "tgs.SetTitle(\"3-Spline interpolation;x\")\n",
    "tl=R.TLegend(0.7,0.7,0.9,0.9)\n",
    "tl.AddEntry(grdata,\"samples\",\"le\")\n",
    "tl.AddEntry(tgs,\"Spline\",\"l\")\n",
    "tl.AddEntry(fplot,\"Truth\",\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tgs.Draw(\"al\")\n",
    "grdata.Draw(\"*\")\n",
    "fplot.Draw(\"same,hist,c\")\n",
    "tl.Draw()\n",
    "tc.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Be aware that interpolations only follow fluctuations in the data.  There is no model offering additional constraints as we will find when applying fits to data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
